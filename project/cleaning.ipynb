{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data cleaning and hashtag extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ['id', 'userId', 'createdAt', 'text', 'long', 'lat', 'placeId', 'inReplyTo', 'source', 'truncated', 'Longitude',\\\n",
    "       'Latitude', 'sourceName', 'sourceUrl', 'username', 'screenName', 'followerscount', 'friendscount', 'statusescount',\\\n",
    "       'userLocation']\n",
    "keep_col = ['id', 'userId', 'createdAt', 'text', 'Longitude', 'Latitude', 'username']\n",
    "keep_final = ['id', 'userId', 'createdAt', 'Longitude', 'Latitude', 'username', 'tag', 'at']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['salut', 'ca va?']"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[s.lower() for s in ['SaLuT', 'Ca Va?']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hashtags(text):\n",
    "    \"\"\"Returns the list of all hashtags (e.g. '#hashtag') present in the given text\"\"\"\n",
    "    try:\n",
    "        res = re.findall(r\"#\\w+\", text)\n",
    "        return [s.lower() for s in res]\n",
    "    except:\n",
    "        print(text)\n",
    "        return list()\n",
    "    \n",
    "def get_mentions(t):\n",
    "    \"\"\"Returns the list of all mentions (e.g. '@mention') present in the given text\"\"\"\n",
    "    return re.findall(r\"@\\w+\", t)\n",
    "\n",
    "def add_lines_in_df(lines, dataframe):\n",
    "    df2 =  pd.DataFrame(lines)\n",
    "    df2.columns = col\n",
    "    df2 = df2[keep_col]\n",
    "    df2['tag'] = df2.text.apply(lambda t: get_hashtags(t))\n",
    "    df2['at'] = df2.text.apply(lambda t: get_mentions(t))\n",
    "    df2 = df2[keep_final]\n",
    "    return pd.concat([dataframe, df2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "1800000\n",
      "1900000\n",
      "2000000\n",
      "2100000\n",
      "2200000\n",
      "2300000\n",
      "2400000\n",
      "2500000\n",
      "2600000\n",
      "2700000\n",
      "2800000\n",
      "2900000\n",
      "3000000\n",
      "3100000\n",
      "3200000\n",
      "3300000\n",
      "3400000\n",
      "3500000\n",
      "3600000\n",
      "3700000\n",
      "3800000\n",
      "3900000\n",
      "4000000\n",
      "4100000\n",
      "4200000\n",
      "4300000\n",
      "4400000\n",
      "4500000\n",
      "4600000\n",
      "4700000\n",
      "4800000\n",
      "4900000\n",
      "5000000\n",
      "5100000\n",
      "5200000\n",
      "5300000\n",
      "5400000\n",
      "5500000\n",
      "5600000\n",
      "5700000\n",
      "5800000\n",
      "5900000\n",
      "6000000\n",
      "6100000\n",
      "6200000\n",
      "6300000\n",
      "6400000\n",
      "6500000\n",
      "6600000\n",
      "6700000\n",
      "6800000\n",
      "6900000\n",
      "7000000\n",
      "7100000\n",
      "7200000\n",
      "7300000\n",
      "7400000\n",
      "7500000\n",
      "7600000\n",
      "7700000\n",
      "7800000\n",
      "7900000\n",
      "8000000\n",
      "8100000\n",
      "8200000\n",
      "8300000\n",
      "8400000\n",
      "8500000\n",
      "8600000\n",
      "8700000\n",
      "8800000\n",
      "8900000\n",
      "9000000\n",
      "9100000\n",
      "9200000\n",
      "9300000\n",
      "9400000\n",
      "9500000\n",
      "9600000\n",
      "9700000\n",
      "9800000\n",
      "9900000\n",
      "10000000\n",
      "10100000\n",
      "10200000\n",
      "10300000\n",
      "10400000\n",
      "10500000\n",
      "10600000\n",
      "10700000\n",
      "10800000\n",
      "10900000\n",
      "11000000\n",
      "11100000\n",
      "11200000\n",
      "11300000\n",
      "11400000\n",
      "11500000\n",
      "11600000\n",
      "11700000\n",
      "11800000\n",
      "11900000\n",
      "12000000\n",
      "12100000\n",
      "12200000\n",
      "12300000\n",
      "12400000\n",
      "12500000\n",
      "12600000\n",
      "12700000\n",
      "12800000\n",
      "12900000\n",
      "13000000\n",
      "13100000\n",
      "13200000\n",
      "13300000\n",
      "13400000\n",
      "13500000\n",
      "13600000\n",
      "13700000\n",
      "13800000\n",
      "13900000\n",
      "14000000\n",
      "14100000\n",
      "14200000\n",
      "14300000\n",
      "14400000\n",
      "14500000\n",
      "14600000\n",
      "14700000\n",
      "14800000\n",
      "14900000\n",
      "15000000\n",
      "15100000\n",
      "15200000\n",
      "15300000\n",
      "15400000\n",
      "15500000\n",
      "15600000\n",
      "15700000\n",
      "15800000\n",
      "15900000\n",
      "16000000\n",
      "16100000\n",
      "16200000\n",
      "16300000\n",
      "16400000\n",
      "16500000\n",
      "16600000\n",
      "16700000\n",
      "16800000\n",
      "16900000\n",
      "17000000\n",
      "17100000\n",
      "17200000\n",
      "17300000\n",
      "17400000\n",
      "17500000\n",
      "17600000\n",
      "17700000\n",
      "17800000\n",
      "17900000\n",
      "18000000\n",
      "18100000\n",
      "18200000\n",
      "18300000\n",
      "18400000\n",
      "18500000\n",
      "18600000\n",
      "18700000\n",
      "18800000\n",
      "18900000\n",
      "19000000\n",
      "19100000\n",
      "19200000\n",
      "19300000\n",
      "19400000\n",
      "19500000\n",
      "19600000\n",
      "19700000\n",
      "19800000\n",
      "19900000\n",
      "20000000\n",
      "20100000\n",
      "20200000\n",
      "20213917\n",
      "1102.6378021240234\n"
     ]
    }
   ],
   "source": [
    "file = open(\"../../twitter_dataset/twitter_full.tsv\", encoding=\"utf8\")\n",
    "data = file.readline()\n",
    "j = 0\n",
    "t = time.time()\n",
    "data_list = list()\n",
    "df = None\n",
    "while not data == \"\":\n",
    "    j+=1\n",
    "    l = len(data.split('\\t'))\n",
    "    while l < 20:\n",
    "        data += file.readline()\n",
    "        l = len(data.split('\\t'))\n",
    "    if l == 20:\n",
    "        data_list.append(data.split('\\t'))\n",
    "    if j%100000 == 0:\n",
    "        print(j)\n",
    "        df2 =  pd.DataFrame(data_list)\n",
    "        df2.columns = col\n",
    "        df2 = df2[keep_col]\n",
    "        df2['tag'] = df2.text.apply(lambda x: keep_tags(x))\n",
    "        df2['at'] = df2.text.apply(lambda x: keep_at(x))\n",
    "        df2 = df2[keep_final]\n",
    "        df = pd.concat([df, df2], ignore_index=True)\n",
    "        data_list = list()\n",
    "    \n",
    "    data = file.readline()\n",
    "    if data == \"\":\n",
    "        print(j)\n",
    "        df2 =  pd.DataFrame(d)\n",
    "        df2.columns = col\n",
    "        df2 = df2[keep_col]\n",
    "        df2['tag'] = df2.text.apply(lambda x: keep_tags(x))\n",
    "        df2['at'] = df2.text.apply(lambda x: keep_at(x))\n",
    "        df2 = df2[keep_final]\n",
    "        df = pd.concat([df, df2], ignore_index=True)\n",
    "        d = list()\n",
    "    \n",
    "\n",
    "print(time.time()-t)\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id           20212854\n",
       "userId       20212854\n",
       "createdAt    20212854\n",
       "Longitude    20212854\n",
       "Latitude     20212854\n",
       "username     20157151\n",
       "tag          20212854\n",
       "dtype: int64"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id           20191872\n",
       "userId       20191872\n",
       "createdAt    20191872\n",
       "Longitude    20191872\n",
       "Latitude     20191872\n",
       "username     20136169\n",
       "tag          20191872\n",
       "dtype: int64"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['Longitude'] != '\\\\N'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('../../twitter_dataset/cleaned.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tag = df[(df[\"tag\"].astype(str) != '[]')]\n",
    "df_tag.to_csv('../../twitter_dataset/cleaned_hashtag.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_at = df[(df[\"at\"].astype(str) != '[]')]\n",
    "df_at.to_csv('../../twitter_dataset/cleaned_identification.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Grouping by hashtag\n",
    "\n",
    "The first step of our descriptive data analysis is to group all tweets by hashtags. This will allow us to count find the most popular hashtags, and later agreagate them by time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/ada/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (1,4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../../twitter_dataset/cleaned.csv\", sep=';', encoding=\"cp437\", usecols=range(1,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/ada/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (1,4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df_tag = pd.read_csv(\"../../twitter_dataset/cleaned_hashtag.csv\", sep=';', encoding=\"cp437\", usecols=range(1,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/ada/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (1,4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df_at = pd.read_csv(\"../../twitter_dataset/cleaned_identification.csv\", sep=';', encoding=\"cp437\", usecols=range(1,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_unique_tag_list_and_store():\n",
    "    \"\"\"Compute the list of different hashtags\"\"\"\n",
    "    \n",
    "    concat_list = np.concatenate(df_tag.tag.apply(lambda x : np.array(keep_tags(x))))\n",
    "    unique_tags = np.unique(concat_list)\n",
    "    with open('../../twitter_dataset/unique_hashtags.json', 'w') as outfile:\n",
    "        json.dump(unique_tags.tolist(), outfile)\n",
    "    \n",
    "def load_unique_tag_list():\n",
    "    with open('../../twitter_dataset/unique_hashtags.json', 'r') as infile:\n",
    "        unique_tags = json.load(infile)\n",
    "    return unique_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_unique_tag_list_and_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtag</th>\n",
       "      <th>tweets_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#oneyearinaustralia</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#BayramHediyemizQUARESMA</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#ray</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#tibetaansegroet</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#YouR</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#Meynet</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>#panefarcito</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#IFeelDown</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>#Qrcode</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#lamiatesta</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>#EVPA13</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>#Consultant</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>#sdfc</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>#jabjabjabrighthook</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>#lagomaggiorie</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>#DeformazioneProfessionale</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>#premierjourdelanneeetdeja</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>#imzentrum</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>#NoWordssss</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>#gottemeitschi</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>#settimanadifficile</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>#gelli</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>#ragingbull</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>#straMouccioni</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>#Endsasbegginings</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>#durer</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>#valmasino</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>#KL1960</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>#nonhoamicinormali</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>#benladen</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001654</th>\n",
       "      <td>#Project8</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001655</th>\n",
       "      <td>#casadifamiglia</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001656</th>\n",
       "      <td>#prerentree</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001657</th>\n",
       "      <td>#wenkenhofgesprΣche</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001658</th>\n",
       "      <td>#palazzopepoli</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001659</th>\n",
       "      <td>#zamazal</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001660</th>\n",
       "      <td>#CaribbeanNight</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001661</th>\n",
       "      <td>#financialservices</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001662</th>\n",
       "      <td>#Gerxit</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001663</th>\n",
       "      <td>#ajx</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001664</th>\n",
       "      <td>#Amethi</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001665</th>\n",
       "      <td>#GiveMeFive</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001666</th>\n",
       "      <td>#31SulCampoFateveneUnaRagione</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001667</th>\n",
       "      <td>#Ostern2016</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001668</th>\n",
       "      <td>#condolences</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001669</th>\n",
       "      <td>#LoveTheWayYouLie</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001670</th>\n",
       "      <td>#ILoveMagcon</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001671</th>\n",
       "      <td>#TuTesFaisVioleCommeLeBayern</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001672</th>\n",
       "      <td>#TuningFork</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001673</th>\n",
       "      <td>#heyyou</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001674</th>\n",
       "      <td>#TouringClub</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001675</th>\n",
       "      <td>#BΣrenhⁿtte</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001676</th>\n",
       "      <td>#eyefinity</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001677</th>\n",
       "      <td>#lillobal</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001678</th>\n",
       "      <td>#CestPasPasquOnEstSuisse</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001679</th>\n",
       "      <td>#ULC</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001680</th>\n",
       "      <td>#xapodebit</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001681</th>\n",
       "      <td>#calvintrillin</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001682</th>\n",
       "      <td>#Chechnya</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001683</th>\n",
       "      <td>#Book_1</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1001684 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               hashtag tweets_idx\n",
       "0                  #oneyearinaustralia         []\n",
       "1             #BayramHediyemizQUARESMA         []\n",
       "2                                 #ray         []\n",
       "3                     #tibetaansegroet         []\n",
       "4                                #YouR         []\n",
       "5                              #Meynet         []\n",
       "6                         #panefarcito         []\n",
       "7                           #IFeelDown         []\n",
       "8                              #Qrcode         []\n",
       "9                          #lamiatesta         []\n",
       "10                             #EVPA13         []\n",
       "11                         #Consultant         []\n",
       "12                               #sdfc         []\n",
       "13                 #jabjabjabrighthook         []\n",
       "14                      #lagomaggiorie         []\n",
       "15          #DeformazioneProfessionale         []\n",
       "16          #premierjourdelanneeetdeja         []\n",
       "17                          #imzentrum         []\n",
       "18                         #NoWordssss         []\n",
       "19                      #gottemeitschi         []\n",
       "20                 #settimanadifficile         []\n",
       "21                              #gelli         []\n",
       "22                         #ragingbull         []\n",
       "23                      #straMouccioni         []\n",
       "24                   #Endsasbegginings         []\n",
       "25                              #durer         []\n",
       "26                          #valmasino         []\n",
       "27                             #KL1960         []\n",
       "28                  #nonhoamicinormali         []\n",
       "29                           #benladen         []\n",
       "...                                ...        ...\n",
       "1001654                      #Project8         []\n",
       "1001655                #casadifamiglia         []\n",
       "1001656                    #prerentree         []\n",
       "1001657            #wenkenhofgesprΣche         []\n",
       "1001658                 #palazzopepoli         []\n",
       "1001659                       #zamazal         []\n",
       "1001660                #CaribbeanNight         []\n",
       "1001661             #financialservices         []\n",
       "1001662                        #Gerxit         []\n",
       "1001663                           #ajx         []\n",
       "1001664                        #Amethi         []\n",
       "1001665                    #GiveMeFive         []\n",
       "1001666  #31SulCampoFateveneUnaRagione         []\n",
       "1001667                    #Ostern2016         []\n",
       "1001668                   #condolences         []\n",
       "1001669              #LoveTheWayYouLie         []\n",
       "1001670                   #ILoveMagcon         []\n",
       "1001671   #TuTesFaisVioleCommeLeBayern         []\n",
       "1001672                    #TuningFork         []\n",
       "1001673                        #heyyou         []\n",
       "1001674                   #TouringClub         []\n",
       "1001675                    #BΣrenhⁿtte         []\n",
       "1001676                     #eyefinity         []\n",
       "1001677                      #lillobal         []\n",
       "1001678       #CestPasPasquOnEstSuisse         []\n",
       "1001679                           #ULC         []\n",
       "1001680                     #xapodebit         []\n",
       "1001681                 #calvintrillin         []\n",
       "1001682                      #Chechnya         []\n",
       "1001683                        #Book_1         []\n",
       "\n",
       "[1001684 rows x 2 columns]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_index_with_hashtag(df, hashtag):\n",
    "    return np.where(df.tag.apply(lambda x : (\"'\"+hashtag+\"'\") in x))\n",
    "\n",
    "def group_by_hashtag(df):\n",
    "    unique_tags = load_unique_tag_list()    \n",
    "    output = {}\n",
    "    for hashtag in unique_tags:\n",
    "        output[hashtag] = get_index_with_hashtag(df, hashtag)\n",
    "    output_df = pd.DataFrame.from_dict(output, orient='index')\n",
    "    output_df.reset_index(inplace=True)\n",
    "    output_df.columns = ['hashtag', 'tweets_idx']\n",
    "    return output_df\n",
    "\n",
    "group_hashtags = group_by_hashtag(df_tag.head(100))\n",
    "group_hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtag</th>\n",
       "      <th>tweets_nb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>#fb</td>\n",
       "      <td>962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5412</th>\n",
       "      <td>#fail</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1390</th>\n",
       "      <td>#lift11</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3990</th>\n",
       "      <td>#Endomondo</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>#bosw</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6297</th>\n",
       "      <td>#android</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4510</th>\n",
       "      <td>#sbb</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633</th>\n",
       "      <td>#iphoneography</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2335</th>\n",
       "      <td>#ff</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2692</th>\n",
       "      <td>#esc</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5139</th>\n",
       "      <td>#photo</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6012</th>\n",
       "      <td>#iPad</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>#Egypt</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6886</th>\n",
       "      <td>#SBB</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3559</th>\n",
       "      <td>#FF</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             hashtag  tweets_nb\n",
       "467              #fb        962\n",
       "5412           #fail        216\n",
       "1390         #lift11         91\n",
       "3990      #Endomondo         80\n",
       "973            #bosw         66\n",
       "6297        #android         65\n",
       "4510            #sbb         64\n",
       "633   #iphoneography         60\n",
       "2335             #ff         58\n",
       "2692            #esc         57\n",
       "5139          #photo         54\n",
       "6012           #iPad         52\n",
       "236           #Egypt         47\n",
       "6886            #SBB         45\n",
       "3559             #FF         42"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_hashtags_nb = group_hashtags.copy()\n",
    "group_hashtags_nb['tweets_nb'] = group_hashtags_nb['tweets_idx'].apply(lambda ls: len(ls))\n",
    "group_hashtags_nb.sort_values('tweets_nb', ascending=False).drop(['tweets_idx'], axis=1).head(15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

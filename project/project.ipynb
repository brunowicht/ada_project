{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from datetime import date, timedelta, datetime\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import scipy.ndimage.filters\n",
    "import re\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import csv \n",
    "import os\n",
    "import folium\n",
    "import branca.colormap as cm\n",
    "from folium.plugins import MarkerCluster\n",
    "from folium import plugins\n",
    "import ast\n",
    "import math\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "from helper import * # File containig some helper functions\n",
    "from maps import *\n",
    "from plots import *\n",
    "from event_detection import *\n",
    "from event_localization import *\n",
    "from const import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Link to interactive visualisation](https://nbviewer.jupyter.org/github/brunowicht/ada_project/blob/master/project/project.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:gray;padding:10px;margin:10px;color:white\">\n",
    "**Run cell below to load necessary data**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading df_tag...\n",
      "Loading group_hashtags...\n",
      "Loading dic_tag_days...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading df_tag...\")\n",
    "df_tag = pd.read_csv(\"../../twitter_dataset/cleaned_hashtag.csv\", sep=';', encoding=\"utf-8\", low_memory=False)\n",
    "\n",
    "print(\"Loading group_hashtags...\")\n",
    "group_hashtags = pd.read_csv(\"../../twitter_dataset/hashtag_grouped.csv\", sep=\";\", index_col=[0], usecols=[0, 1, 2])\n",
    "group_hashtags.tweets_idx = group_hashtags.tweets_idx.apply(lambda s: ast.literal_eval(s))\n",
    "\n",
    "print(\"Loading dic_tag_days...\")\n",
    "pickle_in = open(\"../../twitter_dataset/dic_tag_days.pickle\",\"rb\")\n",
    "dic_tag_days = pickle.load(pickle_in)\n",
    "pickle_in.close()\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of some constants relative to the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Understanding the dataset\n",
    "\n",
    "Before starting any kind of data analysis, we first have to review the content of our dataset and understand its meaning better. To do this, we use the data sample and the schema provided with the dataset.\n",
    "\n",
    "## 0.1. Dataset structure\n",
    "Let us first read the `schema.txt` file to understand what fields our dataset contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Field name</th>\n",
       "      <th>Type</th>\n",
       "      <th>Specification</th>\n",
       "      <th>Unsigned</th>\n",
       "      <th>Optional field</th>\n",
       "      <th>Format</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id</td>\n",
       "      <td>bigint(20)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UNSIGNED</td>\n",
       "      <td>No</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>userId</td>\n",
       "      <td>bigint(20)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UNSIGNED</td>\n",
       "      <td>No</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>createdAt</td>\n",
       "      <td>timestamp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>0000-00-00 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>text</td>\n",
       "      <td>text</td>\n",
       "      <td>utf8_unicode_ci</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>longitude</td>\n",
       "      <td>float</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>latitude</td>\n",
       "      <td>float</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>placeId</td>\n",
       "      <td>varchar(25)</td>\n",
       "      <td>utf8_general_ci</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>inReplyTo</td>\n",
       "      <td>bigint(20)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UNSIGNED</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>source</td>\n",
       "      <td>int(10)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UNSIGNED</td>\n",
       "      <td>No</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>truncated</td>\n",
       "      <td>bit(1)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>placeLatitude</td>\n",
       "      <td>float</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>placeLongitude</td>\n",
       "      <td>float</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>sourceName</td>\n",
       "      <td>varchar(255)</td>\n",
       "      <td>utf8_general_ci</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sourceUrl</td>\n",
       "      <td>varchar(255)</td>\n",
       "      <td>utf8_general_ci</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>userName</td>\n",
       "      <td>varchar(200)</td>\n",
       "      <td>utf8_general_ci</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>screenName</td>\n",
       "      <td>varchar(200)</td>\n",
       "      <td>utf8_general_ci</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>followersCount</td>\n",
       "      <td>int(10)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UNSIGNED</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>friendsCount</td>\n",
       "      <td>int(10)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UNSIGNED</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>statusesCount</td>\n",
       "      <td>int(10)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UNSIGNED</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>userLocation</td>\n",
       "      <td>varchar(200)</td>\n",
       "      <td>utf8_general_ci</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Field name          Type    Specification  Unsigned Optional field  \\\n",
       "1               id    bigint(20)              NaN  UNSIGNED             No   \n",
       "2           userId    bigint(20)              NaN  UNSIGNED             No   \n",
       "3        createdAt     timestamp              NaN       NaN             No   \n",
       "4             text          text  utf8_unicode_ci       NaN             No   \n",
       "5        longitude         float              NaN       NaN            Yes   \n",
       "6         latitude         float              NaN       NaN            Yes   \n",
       "7          placeId   varchar(25)  utf8_general_ci       NaN            Yes   \n",
       "8        inReplyTo    bigint(20)              NaN  UNSIGNED            Yes   \n",
       "9           source       int(10)              NaN  UNSIGNED             No   \n",
       "10       truncated        bit(1)              NaN       NaN             No   \n",
       "11   placeLatitude         float              NaN       NaN            Yes   \n",
       "12  placeLongitude         float              NaN       NaN            Yes   \n",
       "13      sourceName  varchar(255)  utf8_general_ci       NaN            Yes   \n",
       "14       sourceUrl  varchar(255)  utf8_general_ci       NaN            Yes   \n",
       "15        userName  varchar(200)  utf8_general_ci       NaN            Yes   \n",
       "16      screenName  varchar(200)  utf8_general_ci       NaN            Yes   \n",
       "17  followersCount       int(10)              NaN  UNSIGNED            Yes   \n",
       "18    friendsCount       int(10)              NaN  UNSIGNED            Yes   \n",
       "19   statusesCount       int(10)              NaN  UNSIGNED            Yes   \n",
       "20    userLocation  varchar(200)  utf8_general_ci       NaN            Yes   \n",
       "\n",
       "                 Format  \n",
       "1                  None  \n",
       "2                  None  \n",
       "3   0000-00-00 00:00:00  \n",
       "4                  None  \n",
       "5                   NaN  \n",
       "6                   NaN  \n",
       "7                   NaN  \n",
       "8                   NaN  \n",
       "9                  None  \n",
       "10                 None  \n",
       "11                  NaN  \n",
       "12                  NaN  \n",
       "13                  NaN  \n",
       "14                  NaN  \n",
       "15                  NaN  \n",
       "16                  NaN  \n",
       "17                  NaN  \n",
       "18                  NaN  \n",
       "19                  NaN  \n",
       "20                  NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema = pd.read_table('twitter-swisscom/schema.txt', delimiter='    ', engine='python',\n",
    "                       names=['Field name', 'Type', 'Specification', 'Unsigned', 'Optional field', 'Format'] )\n",
    "schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `schema.txt` contains information about each field in our dataset. We see that there are 20 possible fields for a tweet, but many of them are optional, so most tweets probably haver fewer specified field than that.\n",
    "\n",
    "Here are the fields that will be the most useful:\n",
    "- userId: to know who posted the tweet\n",
    "- createdAt: to know when the tweet was posted\n",
    "- text: content of the tweet\n",
    "- longitude and latitude: to know from where the tweet was posted\n",
    "\n",
    "\n",
    "## 0.2. Dataset contents\n",
    "Now, let us have a look at the actual content of our dataset using the provided sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>createdAt</th>\n",
       "      <th>text</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>placeId</th>\n",
       "      <th>inReplyTo</th>\n",
       "      <th>source</th>\n",
       "      <th>truncated</th>\n",
       "      <th>placeLatitude</th>\n",
       "      <th>placeLongitude</th>\n",
       "      <th>sourceName</th>\n",
       "      <th>sourceUrl</th>\n",
       "      <th>userName</th>\n",
       "      <th>screenName</th>\n",
       "      <th>followersCount</th>\n",
       "      <th>friendsCount</th>\n",
       "      <th>statusesCount</th>\n",
       "      <th>userLocation</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>776522983837954049</th>\n",
       "      <td>735449229028675584</td>\n",
       "      <td>2016-09-15 20:48:01</td>\n",
       "      <td>se lo dici tu... https://t.co/x7Qm1VHBKL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51c0e6b24c64e54e</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46.0027</td>\n",
       "      <td>8.96044</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>http://twitter.com/#!/download/iphone</td>\n",
       "      <td>plvtone filiae.</td>\n",
       "      <td>hazel_chb</td>\n",
       "      <td>146</td>\n",
       "      <td>110</td>\n",
       "      <td>28621</td>\n",
       "      <td>Earleen.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776523000636203010</th>\n",
       "      <td>2741685639</td>\n",
       "      <td>2016-09-15 20:48:05</td>\n",
       "      <td>https://t.co/noYrTnqmg9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4e7c21fd2af027c6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46.8131</td>\n",
       "      <td>8.22414</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>http://twitter.com/#!/download/iphone</td>\n",
       "      <td>samara</td>\n",
       "      <td>letisieg</td>\n",
       "      <td>755</td>\n",
       "      <td>2037</td>\n",
       "      <td>3771</td>\n",
       "      <td>Suisse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776523045200691200</th>\n",
       "      <td>435239151</td>\n",
       "      <td>2016-09-15 20:48:15</td>\n",
       "      <td>@BesacTof @Leonid_CCCP Tu dois t'engager en si...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12eb9b254faf37a3</td>\n",
       "      <td>7.765221e+17</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.2010</td>\n",
       "      <td>5.94082</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>http://twitter.com/download/android</td>\n",
       "      <td>lebrübrü❤</td>\n",
       "      <td>lebrubru</td>\n",
       "      <td>811</td>\n",
       "      <td>595</td>\n",
       "      <td>30191</td>\n",
       "      <td>Fontain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776523058404290560</th>\n",
       "      <td>503244217</td>\n",
       "      <td>2016-09-15 20:48:18</td>\n",
       "      <td>@Mno0or_Abyat اشوف مظاهرات على قانون العمل الج...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30bcd7f767b4041e</td>\n",
       "      <td>7.765216e+17</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>45.8011</td>\n",
       "      <td>6.16552</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>http://twitter.com/#!/download/iphone</td>\n",
       "      <td>عبدالله القنيص</td>\n",
       "      <td>bingnais</td>\n",
       "      <td>28433</td>\n",
       "      <td>417</td>\n",
       "      <td>12262</td>\n",
       "      <td>Shargeyah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776523058504925185</th>\n",
       "      <td>452805259</td>\n",
       "      <td>2016-09-15 20:48:18</td>\n",
       "      <td>Greek night #geneve (@ Emilios in Genève) http...</td>\n",
       "      <td>6.14414</td>\n",
       "      <td>46.1966</td>\n",
       "      <td>c3a6437e1b1a726d</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46.2048</td>\n",
       "      <td>6.14319</td>\n",
       "      <td>foursquare</td>\n",
       "      <td>http://foursquare.com</td>\n",
       "      <td>Alkan Şenli</td>\n",
       "      <td>Alkanoli</td>\n",
       "      <td>204</td>\n",
       "      <td>172</td>\n",
       "      <td>3390</td>\n",
       "      <td>İstanbul/Burgazada</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                userId            createdAt  \\\n",
       "id                                                            \n",
       "776522983837954049  735449229028675584  2016-09-15 20:48:01   \n",
       "776523000636203010          2741685639  2016-09-15 20:48:05   \n",
       "776523045200691200           435239151  2016-09-15 20:48:15   \n",
       "776523058404290560           503244217  2016-09-15 20:48:18   \n",
       "776523058504925185           452805259  2016-09-15 20:48:18   \n",
       "\n",
       "                                                                 text  \\\n",
       "id                                                                      \n",
       "776522983837954049           se lo dici tu... https://t.co/x7Qm1VHBKL   \n",
       "776523000636203010                            https://t.co/noYrTnqmg9   \n",
       "776523045200691200  @BesacTof @Leonid_CCCP Tu dois t'engager en si...   \n",
       "776523058404290560  @Mno0or_Abyat اشوف مظاهرات على قانون العمل الج...   \n",
       "776523058504925185  Greek night #geneve (@ Emilios in Genève) http...   \n",
       "\n",
       "                    longitude  latitude           placeId     inReplyTo  \\\n",
       "id                                                                        \n",
       "776522983837954049        NaN       NaN  51c0e6b24c64e54e           NaN   \n",
       "776523000636203010        NaN       NaN  4e7c21fd2af027c6           NaN   \n",
       "776523045200691200        NaN       NaN  12eb9b254faf37a3  7.765221e+17   \n",
       "776523058404290560        NaN       NaN  30bcd7f767b4041e  7.765216e+17   \n",
       "776523058504925185    6.14414   46.1966  c3a6437e1b1a726d           NaN   \n",
       "\n",
       "                    source  truncated  placeLatitude  placeLongitude  \\\n",
       "id                                                                     \n",
       "776522983837954049       1        NaN        46.0027         8.96044   \n",
       "776523000636203010       1        NaN        46.8131         8.22414   \n",
       "776523045200691200       5        NaN        47.2010         5.94082   \n",
       "776523058404290560       1        NaN        45.8011         6.16552   \n",
       "776523058504925185       3        NaN        46.2048         6.14319   \n",
       "\n",
       "                             sourceName  \\\n",
       "id                                        \n",
       "776522983837954049   Twitter for iPhone   \n",
       "776523000636203010   Twitter for iPhone   \n",
       "776523045200691200  Twitter for Android   \n",
       "776523058404290560   Twitter for iPhone   \n",
       "776523058504925185           foursquare   \n",
       "\n",
       "                                                sourceUrl         userName  \\\n",
       "id                                                                           \n",
       "776522983837954049  http://twitter.com/#!/download/iphone  plvtone filiae.   \n",
       "776523000636203010  http://twitter.com/#!/download/iphone           samara   \n",
       "776523045200691200    http://twitter.com/download/android        lebrübrü❤   \n",
       "776523058404290560  http://twitter.com/#!/download/iphone   عبدالله القنيص   \n",
       "776523058504925185                  http://foursquare.com      Alkan Şenli   \n",
       "\n",
       "                   screenName  followersCount  friendsCount  statusesCount  \\\n",
       "id                                                                           \n",
       "776522983837954049  hazel_chb             146           110          28621   \n",
       "776523000636203010   letisieg             755          2037           3771   \n",
       "776523045200691200   lebrubru             811           595          30191   \n",
       "776523058404290560   bingnais           28433           417          12262   \n",
       "776523058504925185   Alkanoli             204           172           3390   \n",
       "\n",
       "                          userLocation  \n",
       "id                                      \n",
       "776522983837954049           Earleen.   \n",
       "776523000636203010              Suisse  \n",
       "776523045200691200             Fontain  \n",
       "776523058404290560           Shargeyah  \n",
       "776523058504925185  İstanbul/Burgazada  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df = pd.read_csv('twitter-swisscom/sample.tsv', encoding='utf-8', sep='\\t', escapechar='\\\\', \n",
    "                        index_col='id', names=schema['Field name'], quoting=csv.QUOTE_NONE, na_values='N')\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the percentage of `NaN` values for each field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "userId              0.00 %\n",
       "createdAt           0.00 %\n",
       "text                1.06 %\n",
       "longitude          82.31 %\n",
       "latitude           82.31 %\n",
       "placeId             0.00 %\n",
       "inReplyTo          72.55 %\n",
       "source              0.00 %\n",
       "truncated         100.00 %\n",
       "placeLatitude       0.00 %\n",
       "placeLongitude      0.00 %\n",
       "sourceName          0.00 %\n",
       "sourceUrl           0.00 %\n",
       "userName            1.15 %\n",
       "screenName          0.00 %\n",
       "followersCount      0.00 %\n",
       "friendsCount        0.00 %\n",
       "statusesCount       0.00 %\n",
       "userLocation       19.57 %\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df.isnull().sum().apply(lambda s: '{0:.2f} %'.format(100*s/sample_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the `latitude` and `longitude` are quite often `NaN`, whereas the `placeLatitude`and `placeLongitude` fields are always specified. Therefore, we will use the latter fields.\n",
    "\n",
    "Let us visualize the geographical distribution of the tweets in the sample dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swiss_coord = [46.8, 8.2]\n",
    "swiss_map = folium.Map(swiss_coord, zoom_start=8)\n",
    "add_markers_to_map_(sample_df.rename(columns={\"placeLongitude\": \"Latitude\", \"placeLatitude\": \"Longitude\"}), swiss_map, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swiss_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data cleaning and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The only columns we keep before any computation\n",
    "keep_col = ['id', 'userId', 'createdAt', 'text', 'placeLongitude', 'placeLatitude']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have the script for reading the whole dataset, filter the tags and identification in the text and store everything in a dataframe.  \n",
    "The function `get_hashtag(text)` takes text, extract the hashtags (all the hashtags have been lowercased because uppercase does not add any information)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('twitter-swisscom/sample.tsv', encoding='utf-8', sep='\\t', escapechar='\\\\',\n",
    "                 names=schema['Field name'], quoting=csv.QUOTE_NONE, na_values='N')\n",
    "\n",
    "# We only keep the column we are interested in\n",
    "df = df[keep_col]\n",
    "\n",
    "# Extract the hashtags from the text.\n",
    "df['tag'] = df.text.apply(lambda t: get_hashtags(t))\n",
    "\n",
    "# We will not use the text anymore, no need to keep it.\n",
    "df = df.drop(['text'], axis=1)\n",
    "\n",
    "# Rename latitude and longitute for easier future usage.\n",
    "df = df.rename(columns={'placeLongitude': 'Longitude', 'placeLatitude': 'Latitude'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to easily group by day, month or year, we decided to add those three columns to our dataframe, so we can drop the createdAt column which contains also the time (with second precision) of the tweet post, but we will not need it. We still need to set the tweet id as the unique index of tweets to be able to retreive some information in the original dataset if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['day'] = df.createdAt.apply(lambda x : x[:10])\n",
    "df['month'] = df.createdAt.apply(lambda x : x[:7])\n",
    "df['year'] = df.createdAt.apply(lambda x : x[:4])\n",
    "df = df.drop(['createdAt'],axis=1)\n",
    "df = df.set_index('id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then store the resulting dataframe in a new csv file which is around 3 times smaller than the original one. We can then filter tweet that contains at least one hashtag and thos which contains at least one identification to create subdataset as we want to work with hashtags and identifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../../twitter_dataset/cleaned.csv', sep=';', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tag = df[(df[\"tag\"].astype(str) != '[]')]\n",
    "df_tag.to_csv('../../twitter_dataset/cleaned_hashtag.csv', sep=';', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Manipulation\n",
    "## 2.1. Grouping by hashtag\n",
    "\n",
    "The first step of our descriptive data analysis is to group all tweets by hashtags. In other words, we want to have a dataframe containing for each hashtag the indices of the tweets in which it appears. This will allow us to count find the most popular hashtags, and later agregate them by time.\n",
    "\n",
    "First, we retrieve our cleaned dataset containing tweets with hashtags by loading it from the file we save in the previous step. This allows us to avoid unnecessary computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_tag = pd.read_csv(\"../../twitter_dataset/cleaned_hashtag.csv\", sep=';', encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can implement the main function that will group our data by hashtag as described above.\n",
    "\n",
    "For the sake of clarity, the following functions were moved in the `helper.py` file:\n",
    "- `get_hashtags(text, lowercase=True)` : Returns the list of all hashtags present in the given text.\n",
    "- `get_index_with_hashtag(df, hashtag)` : Returns the indices of the tweets in which the given hashtag appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_hashtag(df):\n",
    "    \"\"\"For each hashtag, give the indices of the tweets in which it appears.\n",
    "    \n",
    "    df: dataframe to use for the grouping\n",
    "    load_hashtags_list: if True, loads the list of different hashtags from a file saved previously.\n",
    "                        if False, computes the hashtag list again.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the indices of the tweets in which each hashtag appears\n",
    "    output = {}\n",
    "    for index, item in df.iterrows():\n",
    "        tags = get_hashtags(item.tag, lowercase=False)\n",
    "        for tag in tags:\n",
    "            if(tag in output):\n",
    "                output_tag = output[tag]\n",
    "                output_tag[0].append(index)\n",
    "                output_tag[1].append(item.userId)\n",
    "            else:\n",
    "                output[tag] = [[index], [item.userId]]\n",
    "    \n",
    "    for tag, lists in output.items():\n",
    "        output_tag = output[tag]\n",
    "        output_tag[0] = list(np.unique(np.array(lists[0])))\n",
    "        output_tag[1] = len(np.unique(np.array(lists[1])))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Convert to dataframe\n",
    "    output_df = pd.DataFrame.from_dict(output, orient='index')\n",
    "    output_df.reset_index(inplace=True)\n",
    "    output_df.columns = ['hashtag', 'tweets_idx', 'nb_unique_authors']\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can actually call this function on our dataset. We store the result in a csv file so that we won't have to compute it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compute the groups by hashtag\n",
    "group_hashtags = group_by_hashtag(df_tag)\n",
    "group_hashtags = group_hashtags.set_index(\"hashtag\")\n",
    "\n",
    "# Save result to csv file\n",
    "group_hashtags.to_csv(\"../../twitter_dataset/hashtag_grouped.csv\", sep=\";\", encoding=\"utf-8\", columns=[\"tweets_idx\", \"nb_unique_authors\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read hashtag groups from file\n",
    "group_hashtags = pd.read_csv(\"../../twitter_dataset/hashtag_grouped.csv\", sep=\";\", index_col=[0], usecols=[0, 1, 2])\n",
    "group_hashtags.tweets_idx = group_hashtags.tweets_idx.apply(lambda s: ast.literal_eval(s))\n",
    "group_hashtags.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have for each hashtag of our dataset the indices of the tweets in which it appears. With that data, we can for example see what hashtags were tweeted the most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the occurence of each hashtag\n",
    "group_hashtags['tweets_nb'] = group_hashtags['tweets_idx'].apply(lambda ls: len(ls))\n",
    "\n",
    "# Get the 1510most popular hashtags\n",
    "group_hashtags.sort_values('tweets_nb', ascending=False).drop(['tweets_idx'], axis=1).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Analysis and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Visualizing hashtag frequency\n",
    "We want to be able to determine if a certain hashtag has a spike of popularity at a certain time. To do that, we visualize the number of tweets containing a given hashtag per unit of time (day, month, or year)\n",
    "\n",
    "For the sake of clarity, the following functions were moved in the `helper.py` and `plot.py` file respectively:\n",
    "- `search_hashtag(hashtag, df)` : Filter the given dataset to keep only elements that contain the given hashtag.\n",
    "- `plot_frequency_tags(df, col, hashtag, n)` : Display a bar plot of the number of tweets with the given hashtag per unit of time given in 'col' (day, month or year) .\n",
    "\n",
    "For example, let us take a look at the tweets with the hashtag '#jesuischarlie'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_Charlie = search_hashtag(\"#jesuischarlie\", df_tag, group_hashtags)\n",
    "df_Charlie.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_frequency_tags(df_tag, 'day', \"#charliehebdo\", 100, group_hashtags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now try this for events that take place every year at the same period, for example Eurovision and Paléo Festival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_frequency_tags(df_tag, 'month', \"#eurovision\", 30, group_hashtags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Eurovision, we can clearly see a spike each year during the month of may, which is indeed when the contest takes place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_frequency_tags(df_tag, 'day', \"#paleo\", 100, group_hashtags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Paléo Festival, we can again see a spike each year during the end of month of July, but this time we can also see that it lasts about a week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Geographic event localisation\n",
    "\n",
    "Here we can use the geographic information of the tweets to determine the location of an event. To do so, we display on a map the geolocation of each tweet that mentions a given hashtag. The following function were implemented in order to create this map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us try this with Eurovision and Paléo Festival, which were already used as examples previously. As we can see in the maps below, Eurovision is an international event since the geographic repartitions of tweet is quite homogenous over Switzerland. On the other hand, if we look at the hashtag '#paleo', we can quickly see that it takes place in Nyon because there is a very high concentration of tweets about Paléo in that area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_map_with_hasthtag(\"#eurovision\", df_tag, group_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_map_with_hasthtag(\"#paleo\", df_tag, group_hashtags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Event detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Filtering out irrelevant hashtags\n",
    "\n",
    "Currently, we have way too many hashtags and it would be unfeasible run our event detection algorithm on every single one of them. Thus we have to filter out hashtags that are very unlikely to be detected as events by our algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Total number of hashtags : %s\" % group_hashtags.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we filter our hashtags by keeping only the ones that are used in more than **100** tweets. Then remove hashtags tweeted by less than **50** unique users. By filtering with respect to the number of unique users instead of the total number of tweets, we discard hashtags that were tweeted many times by only a few number of people. This could be the case if one or a few twitter bots tweet some hashtag an insane number of times in a short period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_daily_unique_authors(hashtag_dic_days):\n",
    "    return max(hashtag_dic_days.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_df_filtered = group_hashtags.copy()\n",
    "group_df_filtered = group_df_filtered[group_df_filtered['tweets_nb'] > 100]\n",
    "group_df_filtered = group_df_filtered[group_df_filtered['nb_unique_authors'] > 50]\n",
    "\n",
    "group_df_filtered['max_unique_author_day'] = group_df_filtered.index.map(lambda hashtag: get_max_daily_unique_authors(dic_tag_days.get(hashtag)))\n",
    "group_df_filtered[group_df_filtered['max_unique_author_day'] > 5].shape[0]\n",
    "\n",
    "group_df_filtered.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we compute the number of unique authors per days for each filtered hashtags. The output is a dictionary with each hashtags as entries. The value of each entry is itself a dictionary with, dates as keys and number of unique authors as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute dictionnary of hashtags occurences per day\n",
    "dic_tag_days = get_unique_author_per_day(group_df_filtered, \"day\", df_tag)\n",
    "\n",
    "# Save dictionnary to file\n",
    "print('\\nSaving dictionnary to file...')\n",
    "pickle_out = open(\"../../twitter_dataset/dic_tag_days.pickle\",\"wb\")\n",
    "pickle.dump(dic_tag_days, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dictionnary of hashtags occurences per day from file\n",
    "pickle_in = open(\"../../twitter_dataset/dic_tag_days.pickle\",\"rb\")\n",
    "dic_tag_days = pickle.load(pickle_in)\n",
    "pickle_in.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Event Detection algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we have our main detection algorithm.\n",
    "Each function is explained in our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_event_score(day, dic_hashtag, moving_avg, event_kernel, max_factor=2):\n",
    "    \"\"\"Returns the event score for a given day.\n",
    "       This is obtained by innerproduct between the number of unique authors and the given kernel, \n",
    "       and then dividing by the given moving average (which is an average over values around that day).\n",
    "    \"\"\"\n",
    "    # Get the number of unique authors for the given day\n",
    "    day_value = dic_hashtag.get(str(day), 0)\n",
    "    \n",
    "    # If no tweets that day, the score is 0\n",
    "    if day_value == 0:\n",
    "        event_score = 0\n",
    "    else:\n",
    "        # Compute the inner product of the values with the given kernel centered around the day of interest\n",
    "        h = int(len(event_kernel) / 2)\n",
    "        event_inner_prod = 0\n",
    "        for i in range(-h, h+1):\n",
    "            d = day + timedelta(i)\n",
    "            # For the inner product, we limit the values used to max_factor*day_value\n",
    "            value_i = min(max_factor*day_value, dic_hashtag.get(str(d), 0))\n",
    "            event_inner_prod += event_kernel[i + h] * min(max_factor*day_value, dic_hashtag.get(str(d), 0))\n",
    "        \n",
    "        # Divide the innerproduct by the moving average to get the event score\n",
    "        event_score = event_inner_prod / max(1, moving_avg)\n",
    "    \n",
    "    return event_score\n",
    "\n",
    "def detect_event(hashtag, dic_tag_days, threshold=4, averaging_window=35):\n",
    "    \"\"\"Detect potential events related to the given hashtag. \n",
    "       Returns the detected event dates, and an array of event scores for each day in the dataset.\n",
    "    \"\"\"\n",
    "    event_kernel = [0.1, 0.8, 0.1]\n",
    "    dic_hashtag = dic_tag_days.get(hashtag)\n",
    "    \n",
    "    # Create the array containing every day included in the dataset\n",
    "    dates = np.asarray([DATASET_BEGIN_DATE + timedelta(days=i) for i in range(DATASET_TOTAL_DAYS)])\n",
    "    \n",
    "    # Compute the the average and variance for the first moving window\n",
    "    moving_avg_window_end = DATASET_BEGIN_DATE + timedelta(days=averaging_window - 1)\n",
    "    moving_avg = get_average(hashtag, dic_tag_days, DATASET_BEGIN_DATE, moving_avg_window_end)\n",
    "    half_window = int(averaging_window / 2)\n",
    "        \n",
    "    # Compute the event score for each day\n",
    "    event_score = np.zeros(len(dates)) \n",
    "    for i in range(half_window, len(dates) - half_window):\n",
    "        if i != half_window:\n",
    "            moving_avg_update = (dic_hashtag.get(str(dates[i+half_window]),0) - dic_hashtag.get(str(dates[i-half_window-1]),0)) / averaging_window\n",
    "            moving_avg = moving_avg + moving_avg_update\n",
    "        \n",
    "        event_score[i] = get_event_score(dates[i], dic_hashtag, moving_avg, event_kernel)\n",
    "        \n",
    "    # Get the dates at which an event was detected\n",
    "    event_dates = dates[np.where(event_score >= threshold)]\n",
    "    event_dates = [str(d) for d in event_dates]\n",
    "    \n",
    "    return (event_dates, event_score)\n",
    "\n",
    "def detect_events(dic_tag_days, threshold=4, averaging_window=35):\n",
    "    \"\"\"Detect events from our filtered hashtags. \n",
    "       Returns the detected event hashtags with the respecting event dates.\n",
    "    \"\"\"\n",
    "    event_dic = {}\n",
    "    nb_tags = len(dic_tag_days.keys())\n",
    "    print(\"Detecting events...\")\n",
    "    \n",
    "    for tag_idx, tag in enumerate(dic_tag_days.keys()):\n",
    "        detected_events = detect_event(tag, dic_tag_days, threshold=threshold, averaging_window=averaging_window)[0]\n",
    "        \n",
    "        if len(detected_events) > 0:\n",
    "            event_dic[tag] = detected_events\n",
    "        \n",
    "        sys.stdout.write(\"\\r{0:.2f}%\".format((float(tag_idx+1)/nb_tags)*100))\n",
    "        sys.stdout.flush()\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    return event_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "event_dic = detect_events(dic_tag_days)\n",
    "\n",
    "print(\"Events were found for {} hashtags out of {}\".format(len(event_dic.keys()), len(dic_tag_days.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we print 10 event we detected and their dates. Note that there is no particular order for events in our dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(event_dic.items())[40:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Events may span over multiple days, therefore here we decide to group them together if they are close in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_event_dic = group_events_by_date(event_dic)\n",
    "\n",
    "nb_events = np.asarray([len(dates_groups) for dates_groups in grouped_event_dic.values()]).sum()\n",
    "print(\"{} different events were found \".format(nb_events))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we print 10 event we detected and their dates grouped. We can see that sometime the event span multiple dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(grouped_event_dic.items())[40:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a visualisation on how our algorithm decides if it detected an event.\n",
    "For instance, we can look at #christmas. We expect that people will talk progressively more about christmas up to the 25 december. In the graph below, we see this behaviour exactly with a spike on the 25 december. The dashed gray line represent our threashold on the detection. If the score (red line) crosses this threashold, we consider the tag as an event for the given day. We will also plot every dates detected for the hashtag below the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag = \"#christmas\"\n",
    "threshold = 4\n",
    "(event_dates, event_score) = detect_event(hashtag, dic_tag_days, threshold=threshold)\n",
    "plot_hashtag_and_event_score(hashtag, event_score, dic_tag_days, date(2015, 11, 1), date(2015, 12, 31), threshold=threshold)\n",
    "event_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This type of event is something unplaned. The number of unique authors goes up suddenly and then decreases progressively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hashtag = \"#jesuischarlie\"\n",
    "threshold = 6\n",
    "(event_dates, event_score) = detect_event(hashtag, dic_tag_days, threshold=threshold)\n",
    "plot_hashtag_and_event_score(hashtag, event_score, dic_tag_days, date(2015, 1, 1), date(2015, 1, 30), threshold=threshold)\n",
    "event_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hashtag = \"#paleo\"\n",
    "threshold = 4\n",
    "(event_dates, event_score) = detect_event(hashtag, dic_tag_days, threshold=threshold)\n",
    "plot_hashtag_and_event_score(hashtag, event_score, dic_tag_days, date(2014, 7, 10), date(2014, 7, 31), threshold=threshold)\n",
    "event_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Event localization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now try to detect the location of local events. The map below show with a red circle the detected event location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag = '#paleo'\n",
    "dates = [datetime.strptime(d, \"%Y-%m-%d\").date() for d in event_dic[hashtag]]\n",
    "display_event_map(hashtag, dates, df_tag, group_hashtags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this map, we do not have a local event (which can be seen because of the mean deciation). If we try to estimate the position of the event anyway, we see that the location does not make any sens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag = '#riprobinwilliams'\n",
    "dates = [datetime.strptime(d, \"%Y-%m-%d\").date() for d in event_dic[hashtag]]\n",
    "display_event_map(hashtag, dates, df_tag, group_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_locations = get_events_locations(grouped_event_dic, df_tag, group_hashtags, return_mean_std=False)\n",
    "\n",
    "print(\"Locations were found for {} events out of {}\".format(len(events_locations), len(grouped_event_dic.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we print the local events dates and their location.\n",
    "Better visualisation will follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(events_locations.items())[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we have a hashtags multiple times at the same location, we can group it as a single event in our map. This way we will not have multiple markers for the same event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_events_locations = group_by_location(events_locations)\n",
    "list(grouped_events_locations.items())[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------\n",
    "Below is our map for local events. Each marker represent a specific local event. The map is interactive and clicking on a marker will display the hashtag and dates of the event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_events_map(grouped_events_locations, event_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
